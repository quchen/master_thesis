\chapter{Introduction}


\section{Overview in German}

\TODO{deutsche zusammenfassung}



\section{Statistical physics and entropy}

Statistical physics is a branch of physics that uses probability theory in order to approximate and solve problems typically involving large amounts of actors. Due to the generality of these principles, the methods and results are far from being confined to the domain of physics, where the ``actors'' are typically things like single atoms, often revealing surprising similarities between seemingly unrelated fields. To give a few examples: Opinions spread in a fashion that resembles spins in an atomic lattice. Cells contain proteins that, unfolded to full length, would not fit at all and make them explode; a key reason why this never happens also explains why the all the air in the room you're sitting in does not suddenly move in the bottom right corner simultaneously. Fluids and traffic patterns are very much alike in many respects. All these examples have one thing in common: the basic principles are very simple, and for that reason it is found in many places in nature.

The quantity this thesis revolves around is entropy (which accounts for the previous protein example). Its concept dates back all the way to the 19th century, where it became one of the key quantities in classical thermodynamics; probably the most famous mentioning of entropy nowadays is the then conceived ``second law (of thermodynamics)'', which in various formulations expresses the phenomenon that things in nature prefer to break rather than be created, unless a considerable amount of energy is invested. In this sense, (the change of) entropy is a measure of reversibility of a process. Mix two fluilds and stir, then no amount of ``unstirring'' will result in them separating again.

There is another way of approaching entropy though, closely related to computer science and mathematics. Here the so-called Shannon entropy measures how much information is contained in a system, or better: how much about it is unknown. Take a bag of Scrabble pieces: the chance to draw a couple of letters that allow making a word is pretty good, the entropy in the bag is high. Now take a bag of equal size that contains only the letters X, U and F. Drawing from this bag is not going to yield any surprise combinations of useful letters; the system is known to be rigged towards three letters, the entropy is low (and incidentially also the chance to create a useful word).

What scientsts then found out is that both the thermodynamic and the information theoretical entropies are the same thing, just viewed from different angles: thermodynamics deals with systems as a whole, information theory with individual parts on a microscopic level. The intersection of these two is precisely where statistical physics, and therefore this thesis, is located.



\section{Thesis overview}

This thesis is divided in two parts, the first one being about environmental entropy in a discrete, the second one in a continuous setting. Although both parts are independent research subjects, they do share certain ideas and definitions. For this reason some issues in chapter~\ref{chap:flow} are assumed to be known due to being introduced in chapter~\ref{chap:thingie}. Both of these chapters will provide their own overviews at the beginning.


Chapter~\ref{chap:thingie} is the result of playing around in a computer simulation, leading to a surprising outcome: when the resulting histogram was weighted using a seemingly arbitrary factor, it turns out to obey a certain symmetry called detailed fluctuation theorem. A detailed fluctuation theorem is an equation of the form
%
\begin{equation}
	p(x) = e^{x}p(-x)
\end{equation}
%
that is fulfilled by a distribution \(p\) for all values \(x\).

This distribution will be the likelihood to obtain a certain value of one special form of entropy, for which no fluctuation theorems were previously known. Deeper investigation of the phenomenon lead to the discovery of an entire new class of fluctuation theorems, and as a possible application a model for how sudden temperature changes affect the energy flow from/to the environment of a simple system.





The second topic is discussed in chapter~\ref{chap:flow}, which is concerned with how entropy can be defined in a meaningful way in a continuous setting. This is surprisingly difficult, as one could naively think the definitions just carry over. To give a motivating example, consider the before mentioned definition of (Shannon) entropy for a discrete distribution,
%
\begin{equation}
	S_\text{discrete} = -\sum_ip_i\ln p_i
\end{equation}
%
and make it continuous by scaling down the intervals over which \(p\) changes its value. In the limit this will result in an integral of the form
%
\begin{equation}
	S_\text{cont.} = -\int_\Omega\d x\,p(x)\ln p(x) ~.
\end{equation}
%
But there is a problem with this equation: it does not have a lower bound. Intuitively, the \(\delta\)~distribution should have no entropy, as it is the continuous version of a single-valued state. However, inserting it into the equation above results in \(S = -\infty\). Take \(\Omega = \Set R\) and a Gaussian with zero mean and standard deviation \(\sigma\), insert everything in the above equation, then the result will be
%
\begin{align}
	p_\sigma(x) &= \frac1{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{x^2}{2\sigma^2}\right) \\
	\Rightarrow S_\text{cont.} &= \ln(\sigma) + \const
\end{align}
%
which is logarithmically divergent for \(\sigma\to0\), the same limit that yields the \(\delta\)~distribution for a Gaussian. Another way of looking at it is that a \(\delta\) centered at a real value \(\mu\) carries infinite information (hence infinite entropy), namely all the decimals of \(\mu\).

This example hints that entropy is not straightforwardly applied to continuous settings. The actual work in chapter~\ref{chap:flow} will be concerned with how the concept of environmental entropy -- in many cases easier to picture as the heat exchange with the environment -- can be brought from its easy discrete definition into a setting where the degrees of freedom are continuous.




