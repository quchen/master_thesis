\chapter{Introduction}


\section{Overview in German}

Die statistische Physik ist ein Teilgebiet der Physik, in welchem mithilfe von wahrscheinlchkeitstheoretischen Methoden Problemstellungen mit typischerweise einer sehr großen Anzahl von Teilnehmern betrachtet werden. Aufgrund der Allgemeinheit dieser Prinzipien sind die Ergebnisse jedoch nicht auf die Physik beschränkt, in welcher die ``Teilnehmer'' typischerweise einzelne Atome sind: in der Soziologie verhalten sich Meinungen ähnlich wie Spins in einem atomaren Gitter; Zellen enthalten potentiell sehr lange Proteine, die ausgefaltet weit zu groß für sie wären, und der Grund dafür ist der identisch mit dem, warum die Luft in einem Zimmer sich nicht spontan ausschließlich in der rechten Hälfte konzentriert; Flüssigkeiten und Verkehrsmuster ähneln sich in vielen Aspekten. All diese Beispiele haben eine Sache gemein: die grundlegenden Prinzipien dahinter sind sehr einfach, und aus diesem Grund an vielen verschiedenen Stellen in der Natur zu finden.

Die Größe mit der sich diese Arbeit beschäftigt ist die Entropie (welche für das vorige Proteinbeispiel verantwortlich ist). Das Konzept geht zurück auf das 19.~Jahrhundert, wo sie eine wichtige Rolle in der klassischen Thermodynamik spielt; wahrscheinlich das bekannteste Auftreten der Entropie heute ist als Bestandteil des ``Zweiten Hauptsatzes (der Thermodynamik)'', welcher das Phänomen beschreibt, dass in der Natur Dinge viel eher zerstört als kreiert werden, sofern man nicht ein gewisses Maß an Energie aufbringt. In diesem Sinne ist (die Änderung der) Entropie ein Maß für die reversibilität eines Prozesses: mischt man zwei Flüssigkeiten und verrührt sie, wird auch beliebiges ``verunrühren'' sie nicht wieder trennen.

Es gibt einen weiteren Zugang zur Entropie, eng verwandt mit Informatik und Mathematik. Hier steht die sogenannte Shannon-Entropie für ein Maß des Informationsgehalts eines Systems, oder besser: wie viel unbekannt ist. Man stelle sich einen Sack voll Scrabble-Spielsteinen vor -- die Wahrscheinlichkeit zufällig ein paar Buchstaben herauszunehmen, die zusammen ein Wort bilden, ist relativ hoch, und ebenso die Entropie im Sack. Ersetzt man alle Steine nun durch solche die vornehmlich X, U und F enthalten, ist die Wahrscheinlichkeit ein sinnvolles Wort zu bilden wesentlich geringer, und so auch für die Entropie.

Eine wichtige Erkennis war nun, dass die thermodynamische und die informationstheoretische Entropie die gleiche Größe, nur von verschiedenen Richtungen betrachtet, sind: die Thermodynamik befasst sich mit dem System als Ganzem, die Informatik mit individuellen Teilen auf mikroskopischem Niveau. Die statistische Physik, und damit diese Arbeit, ist genau der Schnittpunkt dieser beiden Disziplinen.

Die Arbeit besteht aus zwei Hälften. In der ersten (Kapitel \ref{chap:thingie}) geht es um ein neues sog. Fluktuationstheorem, welches eine wichtige Eigenschaft zeitlich fluktuierender Größen beschreibt; in diesem Fall handelt es sich um die Wahrscheinlichkeit, dass sich eine bestimmte Form von Entropie erhöht oder um den gleichen Betrag verringert, für die bisher kein solcher Zusammenhang bekannt war. Weitergehende Forschung führt zur Entdeckung einer ganzen neuen Klasse von Fluktuationstheoremen, und eine mögliche Anwendung dieser ist die Untersuchung der Reaktion von Systemen auf plötzliche Temperaturänderungen.

Die zweite Hälfte, Kapitel~\ref{chap:flow}, versucht die vorige Form von Entropie auf ein kontinuierliches System zu übertragen. Dies stellt sich als überraschend schwer heraus, da die naive Übertragung der aus dem diskreten Fall bekannten Gesetzen zu inkorrekten Ergebnissen führt. Es ist dennoch möglich eine plausible Lösung zu finden, die sich zudem recht nah an bekannten physikalischen Intuitionen orientiert, und die Entwicklung dieser Formel ist das Hauptresultat dieses Abschnittes.



\section{Statistical physics and entropy}

Statistical physics is a branch of physics that uses probability theory in order to approximate and solve problems typically involving large amounts of actors. Due to the generality of these principles, the methods and results are far from being confined to the domain of physics, where the ``actors'' are typically things like single atoms, often revealing surprising similarities between seemingly unrelated fields. To give a few examples: Opinions spread in a fashion that resembles spins in an atomic lattice. Cells contain proteins that, unfolded to full length, would not fit at all and make them explode; a key reason why this never happens also explains why the all the air in the room you're sitting in does not suddenly move in the bottom right corner simultaneously. Fluids and traffic patterns are very much alike in many respects. All these examples have one thing in common: the basic principles are very simple, and for that reason the emergent phenomena can be found in many places in nature.

The quantity this thesis revolves around is entropy (which accounts for the previous protein example). Its concept dates back all the way to the 19th century, where it became one of the key quantities in classical thermodynamics; probably the most famous mentioning of entropy nowadays is the then conceived ``second law (of thermodynamics)'', which in various formulations expresses the phenomenon that things in nature prefer to break rather than be created, unless a considerable amount of energy is invested. In this sense, (the change of) entropy is a measure of reversibility of a process. Mix two fluilds and stir, then no amount of ``unstirring'' will result in them separating again.

There is another way of approaching entropy though, closely related to computer science and mathematics. Here the so-called Shannon entropy measures how much information is contained in a system, or better: how much about it is unknown. Take a bag of Scrabble pieces: the chance to draw a couple of letters that allow making a word is pretty good, the entropy in the bag is high. Now take a bag of equal size that contains mostly the letters X, U and F. Drawing from this bag is not going to yield any surprise combinations of useful letters; the system is known to be rigged towards three letters, the entropy is low (and incidentially also the chance to create a useful word).

What scientsts then found out is that both the thermodynamic and the information theoretical entropies are the same thing, just viewed from different angles: thermodynamics deals with systems as a whole, information theory with individual parts on a microscopic level. The intersection of these two is precisely where statistical physics, and therefore this thesis, is located.



\section{Thesis overview}

This thesis is divided in two parts, the first one being about environmental entropy in a discrete, the second one in a continuous setting. Although both parts are independent research subjects, they do share certain ideas and definitions. For this reason some issues in chapter~\ref{chap:flow} are assumed to be known due to being introduced in chapter~\ref{chap:thingie}. Both of these chapters will provide their own overviews at the beginning.


Chapter~\ref{chap:thingie} is the result of playing around in a computer simulation, leading to a surprising outcome: when the resulting histogram was weighted using a seemingly arbitrary factor, it turns out to obey a certain symmetry called detailed fluctuation theorem. A detailed fluctuation theorem is an equation of the form
%
\begin{equation}
	p(x) = e^{x}p(-x)
\end{equation}
%
that is fulfilled by a distribution \(p\) for all values \(x\).

This distribution will be the likelihood to obtain a certain value of one special form of entropy, for which no fluctuation theorems were previously known. Deeper investigation of the phenomenon lead to the discovery of an entire new class of fluctuation theorems, and as a possible application a model for how sudden temperature changes affect the energy flow from/to the environment of a simple system.





The second topic is discussed in chapter~\ref{chap:flow}, which is concerned with how entropy can be defined in a meaningful way in a continuous setting. This is surprisingly difficult, and na\"ively carrying over the definitions from the discrete setting produces incorrect results. To back this up with a simple example, consider the before mentioned definition of (Shannon) entropy for a discrete distribution,
%
\begin{equation}
	S_\text{discrete} = -\sum_ip_i\ln p_i
\end{equation}
%
and make it continuous by scaling down the intervals over which \(p\) changes its value. In the limit this will result in an integral of the form
%
\begin{equation}
	S_\text{cont.} = -\int_\Omega\d x\,p(x)\ln p(x) ~.
\end{equation}
%
But there is a problem with this equation: it does not have a lower bound. Intuitively, the \(\delta\)~distribution should have no entropy, as it is the continuous version of a single-valued state. However, inserting it into the equation above results in \(S = -\infty\). Take \(\Omega = \Set R\) and a Gaussian with zero mean and standard deviation \(\sigma\), insert everything in the above equation, then the result will be
%
\begin{align}
	p_\sigma(x) &= \frac1{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{x^2}{2\sigma^2}\right) \\
	\Rightarrow S_\text{cont.} &= \ln(\sigma) + \const
\end{align}
%
which is logarithmically divergent for \(\sigma\to0\), the same limit that yields the \(\delta\)~distribution for a Gaussian. Another way of looking at it is that a \(\delta\) centered at a real value \(\mu\) carries infinite information (hence infinite entropy), namely all the decimals of \(\mu\).

This example hints that entropy is not straightforwardly applied to continuous settings. The actual work in chapter~\ref{chap:flow} will be concerned with how the concept of environmental entropy -- in many cases easier to picture as the heat exchange with the environment -- can be brought from its easy discrete definition into a setting where the degrees of freedom are continuous.




