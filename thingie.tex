\chapter{Strong fluctuation theorem for environmental entropy}

The contents of this chapter were published in Physical Review E, ``Strong fluctuation theorem for nonstationary nonequilibrium systems'' \cite{bib:thingie-paper}. The following is a less condensed description of its contents.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preliminaries}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Fluctuation theorems}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Classical thermodynamics is a theory of averages of certain quantities. For example, temperature is a measure for the \emph{mean} energy per degree of freedom of a gas, or the \emph{mean} cost of energy to increase the \emph{mean} entropy by a certain amount. Unfortunately, this approach requires certain quantities to exist a priori, and does not explain their origins. On the other hand, statistical physics typically starts with all individual constituents at the same time, and tries to explain how certain statistical properties -- the mean being just one of them -- emerge. In that sense, fluctuation theorems are about \textbf{taking away the averages} of classical thermodynamics. One of these averages is the entropy of a system \TODO{ref zum Vorspann}, of which we know that it obeys the second law of thermodynamics, typically written as
%
\begin{equation}
	\label{eqn:2nd_law_classical}
	\d S \geq 0 ~,
\end{equation}
%
meaning that \emph{on average} entropy is always increasing.

However, looking at the microscopic properties of a system, it is indeed possible that entropy spontaneously decreases. Fluctuation theorems provide a characteristic expression for the likelihood for certain statistical events to occur in a system. For example in the case of microscopic total entropy changes \(\Delta S\), it can be shown that
%
\begin{equation}
	P(\Delta S) = e^{\Delta S}P(-\Delta S)
\end{equation}
%
which states that the probability of spontaneous entropy increase by \(\Delta S\) is exponentially more likely than a decrease by the same amount; theorems of the above form will be referred to as \textbf{detailed} fluctuation theorems, as the expression yields symmetry information for each point of a distribution individually. In a sense, this is the analogon to the Second~Law from the viewpoint of statistical physics, and indeed it can be shown that \RefEqn{eqn:2nd_law_classical} can be recovered from this formula: integrating both sides yields
%
\begin{equation}
	\label{eqn:average of exp}
	1 = \inflint \d (\Delta S) P(\Delta S) = \inflint \d (\Delta S) e^{\Delta S}P(-\Delta S) = \left\langle e^{-\Delta S} \right\rangle
\end{equation}
%
which, using Jensen's inequality \(\langle e^x\rangle \geq e^{\langle x\rangle}\), implies
%
\begin{equation}
	\langle\Delta S\rangle \geq 0 ~.
\end{equation}
%
Averages of this form are known as an \textbf{integral} fluctuation theorem.

It turns out that this derivation was overly general: in fact, a detailed fluctuation theorem implies arbitrarily many integral fluctuation theorems, since
%
\begin{align}
	\notag
	   \underbrace{\inflint\d x \, P(x) e^{-\frac x2}A(x)}_{\equiv B}
	&= \int\limits_\infty^{-\infty}\d(-x) P(-x) e^{\frac x2}A(-x)
	 = \underbrace{- \inflint\d x \, P(x) e^{-\frac x2}A(x)}_{= -B}
	 = 0
	\\
	\label{eqn:antisymmetric exp average}
	\Rightarrow
	\langle e^{-\frac x2}A(x)\rangle = 0
\end{align}
%
where \(A\) is an arbitrary antisymmetric function (the relation \RefEqn{eqn:average of exp} can be recovered by setting \(A(x) = \sinh(x/2) = \frac12(e^x-e^{-x}\)).

This was but one example of a fluctuation theorem for \(\Delta S\); there are many more for other quantities, as can be seen e.g. in \cite{seifert-review}, and the result of this chapter will be yet another entire class of fluctuation theorems.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Markov jumping processes}
\label{sec:markov process}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

A \emph{jumping process} is a system consisting of a discrete number of states that, governed by certain transition rates or probabilities, transitions from one state to another. A \emph{Markov process} is a special jumping process where the ``next'' state can only depend on the current state, and has no knowledge about its history. In other words, a Markov process is one in which the system does not have a concept of ``momentum'' in the colloquial sense.

One very simple example for a jumping process, of which a more meaningful version will be discussed later in more detail, is the following: consider a single column of particles stacked straight ontop of one another. At a certain rate, a new particle is either desposited to or evaporated from the stack. In this scenario, the height of the stack is the state of the system, and adding or removing particles contitutes a state transition. \TODO{illustration} Now suppose two consecutive depositions would inhibit another one from occurring. This violates the Markov property, because the next state depends on not only the current state, but how the system got there in the first place. On the other hand, if the rates for both addition and subtraction of particles were constant, the system would satisfy the Markov property.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Terminology}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Suppose we have a system of \(N\) states, enumerated by \(i\). Time runs from \(t = 0\) to \(t = T\). At times \(\tau_i\), the system transitions from an old state \(n_i^-\) into a new one. \(c_i^+\). Grouping multiple successive state changes together, we can describe the history of the system using a trajectory through time/state space,
%
\begin{align}
	\gamma &=~
	\newcommand\iarrow[1]{~\underset{\tau_{#1}}\longrightarrow~}
	%
	\cdots
	\iarrow{i-1}
	c_{i-1}
	\iarrow{i}
	c_{i}
	\iarrow{i+1}
	c_{i+1}
	\iarrow{i+2}
	c_{i+2}
	\iarrow{i+3}
	\cdots
	\\
	&= \{c_{i,\tau_i}\}
\end{align}
%
An ``element'' \(c_{i,\tau_i}\) of \(\gamma\) can thus be interpreted as ``the system is in state \(c_i\) starting at time \(\tau_{i}\), until \(\tau_{i+1}\)''. A visualization of the above description can be found in \RefFigure{fig:discrete_jumping_process}.

\begin{figure}[htb]
	\centering
	\include{figures/discrete_jumping_process}
	\caption[]{Visualization of a discrete jumping process. The system, initially in state \(c_0^+\) at time \(t = \tau_0 = 0\), transitions into a new state \(c_1^+\) at time \(\tau_1\), and remains in this state until the next jump occurs. The process then repeats itself, and the resulting trajectory is a representation of the system's history.}
	\label{fig:discrete_jumping_process}
\end{figure}

In a typical system, many such paths can be taken when starting from an initial configuration \(c_0\); depending on the dynamics of the system, the jumps can occur at different times or to different states. Due to the stochastic nature of the system, trajectories most likely occur with different probabilities; these probabilities will be denoted \(p[\{c_{i,\tau_i}\}|c_0]\) with an explicit mentioning of the starting point \(n_0\).


A Markov process on a set of \(N\) states is characterized by a (possibly time-dependent) transition matrix \(w_{c\to c'}\), whose entries are the rates at which the system jumps from one configuration \(c\) to another one \(c'\). Letting this system run for a certain amount of time will result in such a stochastic trajectory \(\gamma\) through the state space, and such trajectories are what this chapter is about.


Given such a process, the probability of a path \(p[\{c_{i,\tau_i}\}|c_0]\), containing a total of \(J\) jumps, can be expressed as a series of ``staying in the same state'' and ``change state'' contributions, \TODO{ref}
%
\begin{equation}
	\label{eqn:discrete path probability}
	p[\{c_{i,\tau_i}\} | c_0]
	= \underbrace{\exp\left(-\int_{\tau_0}^{\tau_1}\d\tau\;r_{c_0^+}(\tau)\right)}_{\text{stay in state } c_0}
	  \prod_{j=1}^J\underbrace{w_{c_j^-c_j^+}(\tau_j)}_{\text{jump}}
	  \underbrace{\exp\left(-\int_{\tau_j}^{\tau_{j+1}}\d\tau\;r_{c_j^+}(\tau)\right)}_{\text{stay in state } c_j}
\end{equation}
%
where \(r_c\) stands for the escape rate from state \(c\),
%
\begin{equation}
	r_c = \sum_{c'\neq c} w_{c\to c'} ~.
\end{equation}
%
Assuming time-independent rates \(w\), \RefEqn{eqn:discrete path probability} reduces to
\begin{equation}
	p[\{c_{i,\tau_i}\} | c_0]
	\equiv
	Q[\gamma]
	=
	e^{-\tau_0r_{c_0}} \left(
		\prod_{j = 1}^J
		w_{c_{j-1}\to c_j}
		e^{-(\tau_j-\tau_{j-1})r_j}
		\right)
\end{equation}










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Entropy in a Markov process}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

For Markov processes, it is often suitable not to look at the total entropy \STot{}, but to split it up in two constituents, namely the system entropy \SSys{} and the environmental entropy \SEnv{}.

The \textbf{system entropy} \(\mathbf{\SSys{}}\) is the Shannon entropy of an ensemble. Running a Markov process, starting with some random initial configuration \(c_0\) drawn using some distribution \(p_0(t=0)\), then after some time \(\Delta t\) the system will be in a new configuration \(c_i\). Repeating the procedure many times will yield a distribution \(p_i(\Delta t)\) of the various \(c_i\) reached. \SSys{} is now the Shannon entropy associated to that distribution:
%
\begin{align}
	\SSys(t) = \sum_i p_i(t)\ln p_i(t)
\end{align}
%
The standard interpretation of this relationship is that \SSys{} is a quantity describing how ``concentrated'' the system is around certain states at a single point in time. For example, if the transition matrix \(w_{c\to c'}\) has a bias to make the system go into a particular configuration \(c_\text{bias}\), \SSys will be lower.

On the other hand, the \textbf{environmental entropy} \(\mathbf{\SEnv{}}\) takes not the system's internal configuration into account, but characterizes how the current configuration has been reached. Each time the system changes its state, \SEnv{} increases by a certain amount,
%
\begin{align}
	\label{eqn:SEnv single jump definition}
	\Delta\SEnv^{c\to c'} = \ln\frac{w_{c\to c'}}{w_{c'\to c}} ~.
\end{align}
%
This provides a characteristic quantity for the reversibility of a process: \(\Delta \SEnv\) is zero if the jumps and its reverse are equally likely, larger than zero if a transition was made in a more likely state, and lastly smaller than zero if an unlikely jump (towards lower uncertainty) happens.

A stochastic trajectory \(\gamma\) now is a series of many such jumps, hence the total of the accumulated environmental entropy can be seen as a functional on \(\gamma\),
%
\begin{align}
	\label{eqn:SEnv rate jump functional}
	\Delta\SEnv[\gamma] = \sum_{i=1}^n\ln\frac{w_{c_{i-1}\to c_i}}{w_{c_i\to c_{i-1}}}
\end{align}
%
where \(n\) is the total number of jumps on \(\gamma\), and \(c_i\) are the configurations it consists of.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Fluctuation theorem for environmental entropy}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


From now on, all rates will be assumed time-independent,
\begin{equation}
	\forall c, c':~ \dot w_{c\to c'} = 0 ~.
\end{equation}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Preliminary definitions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The main result of this section is the derivation of a new fluctuation theorem for \(\Delta\SEnv\). The approach taken will be to first derive an entirely new class of fluctuation theorems, which will contain the desired result as a special case.

In a given Markov process, each allowed path \(\gamma\) appears with a certain probability \(W[\gamma]\). Paired with another functional \(F[\gamma]\) along the path, this allows us to express statistical properties of \(F\) by integrating over all paths; in particular, the average of \(F\) can be written as
%
\begin{align}
	\label{eqn:functional average}
	\langle F\rangle = \int \Cal D \gamma\, W[\gamma] F[\gamma] ~.
\end{align}
%
The path integral ``\(\int \Cal D\gamma\)'' is shorthand notation for summation over all paths \(\gamma = \{c_0, c_1, \ldots, c_n\}\) with \(N\) possible configurations,
%
\begin{align}
	\int \Cal D\gamma F[\gamma]
	\equiv
	\sum_{c_0=1}^N\sum_{c_1=1}^N\cdots\sum_{c_n=1}^N F(c_0, c_1, \ldots c_n) ~.
\end{align}
%
Using this formalism, the probability distribution of increasing environmental entropy can, with the help of the \(\delta\) distribution, be written as a functional integral
%
\begin{equation}
	\begin{split}
	P(\Delta\SEnv = X)
	&= \langle \delta(X-\Delta\SEnv[\gamma])\rangle \\
	&= \int\Cal D\gamma W[\gamma] \delta(X-\Delta\SEnv[\gamma])
	~.
	\end{split}
\end{equation}

For the purpose of the new theorem, the average \RefEqn{eqn:functional average} is modified by introducing a weight functional \(\chi_{c_0,c_T} > 0\) that only depends on the very first and last configuration along the stochastic trajectory, and satisfies the symmetry relation
%
\begin{align}
	\label{eqn:chi symmetry}
	\chi_{c_0,c_T}p_{c_0}^\init = \chi_{c_T,c_0}p_{c_T}^\init
\end{align}
%
where \(p_c^\init\) is the probability that the whole process is started (initialized) with configuration \(c\). Using this, a weighted average can be defined,
%
\begin{align}
	\label{eqn:weighted functional average}
	\langle F\rangle_\chi
	= \frac1{\Cal N} \int \Cal D \gamma\, W[\gamma] F[\gamma] \chi_{c_0,c_T}
\end{align}
%
where \(\Cal N\) is a normalization constant to take the addition of \(\chi\) into account,
%
\begin{align}
	\label{eqn:weighted normalized}
	\Cal N = \int\Cal D\gamma W[\gamma] \chi_{c_0,c_T}
\end{align}


Applied to the environmental entropy production \(\Delta\SEnv\), this weighted average reads
%
\begin{equation}
	\begin{split}
	\label{eqn:P(SEnv)}
	\tilde P(\Delta\SEnv = X)
	&= \langle \delta(X-\Delta\SEnv[\gamma])\rangle_\chi \\
	&= \frac1{\Cal N} \int\Cal D\gamma W[\gamma] \delta(X-\Delta\SEnv[\gamma]) \chi_{c_0,c_T}
	\end{split}
\end{equation}

This allows the main result of this chapter to be stated, namely that the quantity \(\tilde P\) satisfies a detailed fluctuation theorem,
%
\begin{equation}
	\label{eqn:SEnv fluctuation theorem}
	\boxed{
		\tilde P(\Delta\SEnv = X) = e^X \tilde P(\Delta\SEnv = -X)
	}
\end{equation}
%
Note that \(\tilde P\) is still normalized (and therefore a probability distribution, since \(\chi > 0\)); explicitly:
\begin{equation}
	\begin{split}
	   \inflint\d X \tilde P(\Delta\SEnv = X)
	&= \inflint\d X \frac1{\Cal N} \int\Cal D\gamma W[\gamma] \delta(X-\Delta\SEnv[\gamma]) \chi_{c_0,c_T}
	\\
	&=  \frac1{\Cal N} \underbrace{\int\Cal D\gamma W[\gamma] \chi_{c_0,c_T}}_{=\; \Cal N ~ \text{\RefEqn{eqn:weighted normalized}}}
	    \underbrace{\inflint\d X \delta(X-\Delta\SEnv[\gamma])}_{=1}  \\
	&= 1
	\end{split}
\end{equation}
%
In particular, this allows reducing \RefEqn{eqn:SEnv fluctuation theorem} analogous to \RefEqn{eqn:antisymmetric exp average} to
\begin{equation}
	\label{eqn:SEnv asymmetric average}
	\left\langle e^{-\frac12\Delta\SEnv} A(\Delta\SEnv)\right\rangle_\chi = 0
\end{equation}
%
where again \(A\) is an arbitrary antisymmetric function.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of a new class of detailed fluctuation theorems}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Path reversal and environmental entropy}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

While \RefEqn{eqn:P(SEnv)} provides a statistical summary, an expression relating the actual value of \(\Delta\SEnv\) to path probabilities will be useful in the proof. \RefEqn{eqn:SEnv rate jump functional} is such a functional, but is rate- and noth path-based; therefore, the following postulate is useful, and will be shown to be equivalent to \RefEqn{eqn:SEnv rate jump functional}. Note that, although not necessary for this thesis, the statement holds even when the rates are time-dependent.
%
\begin{equation}
	\label{eqn:SEnv path functional}
	\Delta\SEnv
	= \ln\frac{p[\{c_{i,\tau_i}\}|c_0]}{p^\dagger[\{c^\dagger_{i,\tau_i}\}|c^\dagger_0]}
\end{equation}
%
Here, ``\(\dagger\)'' stands for walking a path in the reverse direction. With respect to \RefEqn{eqn:discrete path probability}, this means
%
\begin{enumerate}
	\item Since the path is walked backwards, the boundaries of integration along all paths flip: \(\int_{\tau_i}^{\tau_j} \overset\dagger\longrightarrow \int_{\tau_j}^{\tau_i}\)
	\item Time reverses its direction in general, therefore \(\d\tau \overset\dagger\longrightarrow \d(-\tau) = -\d\tau\), effectively reverting the exchange of integral boundaries mentioned above.
	\item ``\(c_i^-\)'' stands for the state of the system at time \(\tau_i\) before, ``\(c_i^+\)'' for after the jump. Therefore, reversal of time means \(\pm\overset\dagger\longrightarrow\mp\), as states flip the other way round.
\end{enumerate}

With that in mind, for a path with a total of \(J\) jumps
%
\begin{align*}
	&\phantom{=}
		\ln\frac{p[\{c_{i,\tau_i}\} | c_0]}{p^\dagger[\{c^\dagger_{i,\tau_i}\} | c_0^\dagger]}
	\\
	&=
		\ln\frac{
			\exp\left(-\int_{\tau_0}^{\tau_1}\d\tau\;r_{c_0^+}(\tau)\right)
			\prod_{j=1}^J
				w_{c_j^-c_j^+}(\tau_j)
				\exp\left(-\int_{\tau_j}^{\tau_{j+1}}\d\tau\;r_{c_j^+}(\tau)\right)
		}{
			\exp\left(-\int_{\tau_{J+1}}^{\tau_J}\d(-\tau)r_{c_{J+1}^-}(\tau)\right)
			\prod_{j=J}^1
				w_{c_j^+c_j^-}(\tau_j)
				\exp\left(-\int_{\tau_j}^{\tau_{j-1}}\d(-\tau)r_{c_j^-}(\tau)\right)
		}
	\\
	&=
		\ln\frac{
			\exp\left(-\int_{\tau_0}^{\tau_1}\d\tau\;r_{c_0^+}(\tau)\right)
			\prod_{j=1}^J
				\exp\left(-\int_{\tau_j}^{\tau_{j+1}}\d\tau\;r_{c_j^+}(\tau)\right)
		}{
			\exp\left(-\int_{\tau_J}^{\tau_{J+1}}\d\tau\;r_{c_{J+1}^-}(\tau)\right)
			\prod_{j=1}^J
				\exp\left(-\int_{\tau_{j-1}}^{\tau_j}\d\tau\;r_{c_j^-}(\tau)\right)
		} + \ln\prod_{j=1}^J\frac{w_{c_j^-c_j^+}(\tau_j)}{w_{c_j^+c_j^-}(\tau_j)}
	\\
	&=
		\ln\frac{
			\prod_{j=0}^J
				\exp\left(-\int_{\tau_j}^{\tau_{j+1}}\d\tau\;r_{c_j^+}(\tau)\right)
		}{
			\prod_{j=1}^{J+1}
				\exp\left(-\int_{\tau_j}^{\tau_{j-1}}\d\tau\;r_{c_j^-}(\tau)\right)
		}
		+ \underbrace{\sum_{j=1}^J\ln\frac{w_{c_j^-c_j^+}(\tau_j)}{w_{c_j^+c_j^-}(\tau_j)}}_{=\Delta s_m}
	\\
	&=
		\ln\frac{
			\prod_{j=0}^J
				\exp\left(-\int_{\tau_j}^{\tau_{j+1}}\d\tau\;r_{c_j^+}(\tau)\right)
		}{
			\prod_{j=0}^J
				\exp\left(-\int_{\tau_j}^{\tau_{j+1}}\d\tau\;r_{c_{j+1}^-}(\tau)\right)
		}
		+ \Delta s_m
	\\
	&=
		\Delta s_m
\end{align*}
%
as claimed in \RefEqn{eqn:SEnv path functional}. Specialized to time-independent rates, this then reads
\begin{align}
	\label{eqn:SEnv path functional constant rates}
	\Delta\SEnv
	= \ln\frac{Q[\gamma]}{Q[\gamma^\dagger]}
\end{align}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Generalized statement and proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\theorem[Modified master fluctuation theorem]{
\label{thm:thingie}
Let \(F[\gamma] = -F[\gamma^\dagger]\) be an antisymmetric functional on a path \(\gamma\) (generated by a process with time-independent rates), and \(g\) be an arbitrary function acting on it. Then
\begin{equation}
	\boxed{
		\label{eqn:thingie}
		\langle g(F[\gamma])\rangle_\chi
		=
		\langle e^{-\Delta\SEnv[\gamma]}g(-F[\gamma])\rangle_\chi
	}
\end{equation}
} % end theorem

\proof{
(This proof largely follows the same ideas as the one for the Master Fluctuation Theorem presented in \cite{seifert-review}.)

Let
\begin{align}
	W[\gamma] &= p^\init_{c_0}Q[\gamma]
\end{align}
%
The left hand side of \RefEqn{eqn:thingie} can, using \RefEqn{eqn:weighted functional average}, be written as
%
\begin{equation}
	\label{eqn:gF average}
	\langle g(F[\gamma])\rangle_\chi
	=
	\frac1{\Cal N} \int\Cal D\gamma \, p^\init_{c_0}Q[\gamma] g(F[\gamma])\chi_{c_0,c_T} ~.
\end{equation}
%
As the order of summation does not matter (\(\Cal D\gamma = \Cal D\gamma^\dagger\)), instead of integrating along the trajectory \(\gamma\), the direction can be reversed by substituting \(\gamma\to\gamma^\dagger\) and therefore also swapping \(c_0\) and \(c_T\), giving
\begin{equation}
	\langle g(F[\gamma])\rangle_\chi
	=
	\frac1{\Cal N} \int\Cal D\gamma^\dagger \, p^\init_{c_T}Q[\gamma^\dagger] g(F[\gamma^\dagger])\chi_{c_T,c_0} ~.
\end{equation}
%
This purely mathematical step is now for the most part undone using \RefEqn{eqn:SEnv path functional constant rates}, the antisymmetry of \(F\) and the symmetry of \(\chi\) \RefEqn{eqn:chi symmetry},
%
\begin{equation}
	\langle g(F[\gamma])\rangle_\chi
	=
	\frac1{\Cal N} \int\Cal D\gamma \,
		p^\init_{c_T}e^{-\Delta\SEnv[\gamma]}Q[\gamma]
		g(-F[\gamma])\chi_{c_0,c_T} ~.
\end{equation}
%
Compating this result with \RefEqn{eqn:gF average} allows the expression on the right hand side as an average as well, resulting the theorem's claim
%
\begin{equation}
	\langle g(F[\gamma])\rangle_\chi
	=
	\langle e^{-\Delta\SEnv[\gamma]}g(-F[\gamma])\rangle_\chi ~.
\end{equation}
} % end proof


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Specialization to a theorem about environmental entropy}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The theorem just proven is overly general to the point where it is not clear how to apply it. As promised earlier, \RefEqn{eqn:SEnv fluctuation theorem} is a special case of theorem~\RefTheorem{thm:thingie}. First note that environmental entropy is an uneven functional on the trajectory,
\begin{equation}
	  \Delta\SEnv[\gamma]
	= \ln\frac{Q[\gamma]}{Q[\gamma^\dagger]}
	= -\ln\frac{Q[\gamma^\dagger]}{Q[\gamma]}
	= -\Delta\SEnv[\gamma^\dagger] ~.
\end{equation}
%
Second, choose
\begin{equation}
	g(\Delta\SEnv[\gamma]) = \delta(X - \Delta\SEnv[\gamma]) ~,
\end{equation}
%
then \RefEqn{eqn:SEnv fluctuation theorem} allows writing \RefEqn{eqn:thingie} as
%
\begin{align*}
		\langle \delta(X - \Delta\SEnv[\gamma])\rangle_\chi
		&=
		\langle e^{-\Delta\SEnv[\gamma]}\delta(X + \Delta\SEnv[\gamma])\rangle_\chi
	\\ \Leftrightarrow
		\tilde P(\Delta\SEnv = X)
		&=
		e^X \tilde P(\Delta\SEnv = -X)
\end{align*}
%
as claimed.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Applications}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

One of the challenges of general theorems is fixing the degrees of freedom it provides so that a practically viable result emerges. The Master Fluctuation Theorem \cite{seifert-review} does not have this issue as much, because it unifies lots of known fluctuation theorems; in the present case however, the general theorem is proven, but what it unifies or even describes is not so clear. This section will provide an example of one emergent result: a fluctuation theorem for the energy set free after sudden cooling of an equilibrium system.

Take a classical system that is initially in equlibrium with a heat bath of constant temperature \(T_1 = 1/(k_B\beta_1)\). Assuming each configuration \(c\in\Omega\) is associated to a specific energy \(E_c\), the distribution of states in this setting is Boltzmannian,
%
\begin{equation}
	p_c^\init = \frac1{Z(\beta_1)}e^{-\beta_1E_c}
\end{equation}
%
with the partition sum \(Z(\beta) = \sum_ce^{-\beta E_c}\). Since the system is in equlibrium it also obeys detailed balance \TODO{ref}, and consequently the rates can be expressed as follows:
%
\begin{align}
	p_c w_{c\to c'} &= p_{c'} w_{c'\to c}
	\\
	\Leftrightarrow
		   \frac{w_{c\to c'}}{w_{c'\to c}}
		&= \frac{p_{c'}}{p_c}
		 = e^{-\beta_1(E_{c'}-E_c)}
\end{align}

At time \(t = 0\), the temperature of the bath is now instantaneously changed (``quenched'') to \(\beta_2\) \TODO{figure?}, and the system converges to a new stationary equilibrium state. How much energy \(\Delta E = E_{c_0} - E_{c_T}\) does this process take out or feed into the system in a given amount of time \(T\)?
Since the reservoir is not part of the system, changing the temperature leaves the energy associated to a certain state unchanged. What \emph{does} change, however, are the rates, which again fulfill detailed balance, albeit at a different temperature \(T_2 = 1/(k_B\beta_2)\),
%
\begin{equation}
	\frac{w_{c\to c'}}{w_{c'\to c}}
	= e^{-\beta_2(E_{c'}-E_c)}
\end{equation}
%
According to \RefEqn{eqn:SEnv single jump definition}, the logarithm of this expression is precisely the increase in environmental entropy such a transition produces, hence
%
\begin{equation}
	\Delta\SEnv^{c\to c'} = -\beta_2(E_{c'}-E_c) ~.
\end{equation}
%
Since after the quench \(\beta = \beta_2\) is constant, this expression is easily applied jump-wise to \RefEqn{eqn:SEnv rate jump functional}, resulting in the simple functional dependency
%
\begin{equation}
	\label{eqn:Delta SEnv in terms of energies}
	\Delta\SEnv[\gamma] = -\beta_2(E_{c_T}-E_{c_0}) ~.
\end{equation}
%
\TODO{Why does this hold even if the system isn't in equilibrium again yet? That's what our paper says.}

In order to connect this to the new class of fluctuation theorems obtained in the previous section, define the weights
%
\begin{equation}
	\label{eqn:chi example value}
	  \chi_{c_0,c_T}
	= \sqrt{\frac{p_{c_T}^\init}{p_{c_0}^\init}}
	= e^{-\frac12\beta_1(E_{c_T} - E_{c_0})}
	\equiv e^{-\frac12\beta_1\Delta E}
\end{equation}
%
(note how they fulfill the symmetry condition \RefEqn{eqn:chi symmetry}). Inserted into \RefEqn{eqn:SEnv asymmetric average}, writing \(\chi\) as an explicit factor instead of an index to the average, the result is
%
\begin{equation}
	\left\langle e^{-\frac12\Delta\SEnv} A(\Delta\SEnv) \chi_{c_0,c_T} \right\rangle = 0 ~.
\end{equation}
%
Using \RefEqn{eqn:Delta SEnv in terms of energies} this can be expressed in terms of \(\Delta E\):
%
\begin{equation}
	\left\langle e^{+\frac12\beta_2\Delta E} A(-\beta_2\Delta E) \chi_{c_0,c_T} \right\rangle = 0
\end{equation}
%
Upon inserting \(\chi\) \RefEqn{eqn:chi example value}, rescaling \(A\) linearly to \(\tilde A(\Delta E) = A(-\beta_2\Delta E)\) and \(\Delta\beta = \beta_2-\beta_1\), and absorbing \(\Cal N\) by the right hand side's zero, this can be written compactly as
%
\begin{equation}
	\left\langle e^{+\frac12\Delta\beta\Delta E} \tilde A(\Delta E) \right\rangle = 0 ~.
\end{equation}
%
With the special choice \(\tilde A(\Delta E) = \sinh(\frac12\Delta\beta\Delta E)\), a final simple integral fluctuation theorem is obtained:
%
\begin{equation}
	\boxed{\left\langle e^{\Delta\beta \Delta E}\right\rangle = 1}
\end{equation}

\TODO{explain the formula}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Numerical tests}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This section describes two numerical simulations to test the theorems previously developed. The first example will pick up right where the previous section ended with a concrete model, while the second one makes relatively little assumptions about the dynamics of a system, providing a more general perspective.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Simple growth process}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Nonequilibrium model on a small state space}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



